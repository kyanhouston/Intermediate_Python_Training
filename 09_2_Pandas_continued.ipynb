{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Soccer-Stats-Premier-League.xlsx\")\n",
    "print(\"Soccer-Stats-Premier-League.xlsx data\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "print(\"Selecting multiple columns, we pass a list of columns\")\n",
    "print(df[['Team', 'Win', 'GD']], \"\\n\")\n",
    "\n",
    "print(\"Slicing rows and columns(rows=3, col 0-4, excluding 4):\")\n",
    "print(df.iloc[:3, 0:4], \"\\n\")\n",
    "\n",
    "print(\"retrieving multiple rows by iloc method\");\n",
    "print(df.iloc[[2, 4, 6]], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation and grouping of data\n",
    "\n",
    "* We can create a grouping of categories and apply a function to the categories (useful for data analysis). concept. \n",
    "\n",
    "* Groupby mainly refers to a process involving one or more of the following steps:  \n",
    "\n",
    "    * Splitting : It is a process in which we split data into group by applying some conditions on datasets.\n",
    "    * Applying : It is a process in which we apply a function to each group independently\n",
    "    * Combining : It is a process in which we combine different datasets after applying groupby and results into a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# Define a dictionary containing employee data\n",
    "data = {'Name':['Bob', 'Anuj', 'Jai', 'David','Bob', 'Anuj', 'David', 'Abhi'],\n",
    "        'Age':[27, 24, 22, 32, 33, 26, 27, 32],\n",
    "        'Salary':[200, 150, 140, 250, 300, 200, 150, 200],\n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd',\n",
    "                         'B.Tech', 'B.com', 'Msc', 'MA']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using Aggregate Function sum\")\n",
    "result = df[['Age', 'Salary']].aggregate('sum')\n",
    "print(result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using groupby() and aggregate()\")\n",
    "result = df.groupby('Name')['Age'].aggregate(['count', 'size', 'sum', 'mean', 'min', 'max', 'first', 'last'])\n",
    "print(result, \"\\n\")\n",
    "\n",
    "print(\"Using groupby() and aggregate() alternate way\")\n",
    "result = df[['Name', 'Age', 'Salary']].groupby('Name').aggregate(['count', 'sum', 'mean', 'min', 'max'])\n",
    "print(result, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"groupby() multiple columns and multiple aggregations\")\n",
    "result = df.groupby('Name').aggregate({'Age': 'count', 'Salary':['mean', 'min', 'max']})\n",
    "print(result, \"\\n\")\n",
    "\n",
    "print(\"Returns different statistics\")\n",
    "result = df[['Age']].describe()\n",
    "print(result, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping, transforming, and cleaning of data\n",
    "\n",
    "* melt() is used to convert a wide dataframe into a longer form. \n",
    "\n",
    "    * pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name=’value’, col_level=None)\n",
    "\n",
    "* Pivoting, Unmelting or Reverse Melting is used to convert a column with multiple values into several columns of their own.\n",
    "\n",
    "    * DataFrame.pivot(index=None, columns=None, values=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "print(\"creating a weather dataframe from csv\")\n",
    "df = pd.read_csv(\"weather.csv\")\n",
    "print(df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df melt\")\n",
    "dfMelt = pd.melt(df, id_vars = ['Day'])\n",
    "print(dfMelt, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df melt with var_name and value_name\")\n",
    "dfMelt = pd.melt(df, id_vars = ['Day'], var_name = \"City\", value_name=\"Temperature\")\n",
    "print(dfMelt, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df melt with filter\")\n",
    "dfFilter = dfMelt[dfMelt['City'] == 'Delhi']\n",
    "print(dfFilter, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and initializing a list\n",
    "values = [[101, 'Ravi', 345, 'Football'],\n",
    "          [111, 'Bob', 200, 'Chess'],\n",
    "          [201, 'David', 250, 'Football'],\n",
    "          [111, 'Tom', 300, 'Badminton'],\n",
    "          [123, 'Ajay', 460, 'Badminton']]\n",
    "\n",
    "print(\"creating a pandas dataframe - Students data\")\n",
    "df = pd.DataFrame(values, columns=['ID', 'Name', 'Marks', 'Sport'])\n",
    "print(df, \"\\n\")\n",
    "\n",
    "print(\"pivot sorts on index attribute\")\n",
    "dfReshaped = df.pivot(index='Name', columns='Sport')\n",
    "print(dfReshaped, \"\\n\")\n",
    "\n",
    "print(\"reseting index\")\n",
    "df1 = dfReshaped.reset_index()\n",
    "print(df1, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning means fixing bad data in your data set:\n",
    "\n",
    "* Bad data could be:\n",
    "* Empty cells\n",
    "* Data in wrong format\n",
    "* Wrong data\n",
    "* Duplicates\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# marks.csv file contents\n",
    "\n",
    "Id,Name,Marks,Subject\n",
    "1,Ravi,89,Math\n",
    "2,Subbu,30,Math\n",
    "2,Subbu,30,Math\n",
    "3,Ajay,Ninety,Math\n",
    "4,,50,Math\n",
    "5,Satyam,,Math\n",
    "6,David,-30,Math\n",
    "7,Rob,89,NaN\n",
    "\n",
    "# The data set contains some empty cells (Name for Id 4 and Marks for ID 5) - Id and Marks cannot be empty\n",
    "# The data set contains wrong format (Marks for ID 3) - Data should be numeric\n",
    "# The data set contains wrong data (Marks for ID 6) - No negative marks allowed\n",
    "# The data set contains duplicates (ID 2) - Duplicates not allowed for Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "print(\"creating marks dataframe from csv\")\n",
    "df = pd.read_csv(\"marks.csv\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "print(\"removing duplicates\")\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we can use mean, median or mode to replace it with \n",
    "rep = df[\"Marks\"].mode()  # occurs maximum times\n",
    "print(\"Fill empty cells for specific column\")\n",
    "df[\"Marks\"].fillna(rep[0], inplace = True)\n",
    "print(df, \"\\n\")\n",
    "\n",
    "print(\"Drop empty cells\")\n",
    "df.dropna(inplace = True)\n",
    "print(df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remove records which have value where marks is non numeric\")\n",
    "lst=[]\n",
    "j=0\n",
    "for i in df.Marks:\n",
    "  if i.isalpha():\n",
    "    lst.append(j)\n",
    "  j=j+1\n",
    "\n",
    "print(\"Marks which have non numeric data\")\n",
    "print(lst, \"\\n\")\n",
    "\n",
    "df = df.reset_index()\n",
    "print(\"After resetting index\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "df=df.drop(lst, axis=0)\n",
    "print(\"After dropping records which have value where marks is non numeric\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "# Drop negative values in Marks column\n",
    "print(\"After dropping negative marks records\")\n",
    "df.Marks=df.Marks.astype('int32',)\n",
    "df=df[df.Marks>=0]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping data: web API, parsing html and XML, JSON \n",
    "\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites.\n",
    "\n",
    "    * pip install lxml\n",
    "    If the installation fails do the following\n",
    "    \n",
    "    Download the correct version of lxml file from following URL    \n",
    "    https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml\n",
    "    \n",
    "    I chose the following which is compatible with my OS and Python version    \n",
    "    lxml-4.9.0-cp311-cp311-win_amd64.whl\n",
    "    \n",
    "    * pip install lxml-4.9.0-cp311-cp311-win_amd64.whl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open('sample.html', 'r') as f:\n",
    "  html_string = f.read()\n",
    " \n",
    "df = pd.read_html(html_string)\n",
    "print(df, \"\\n\")\n",
    "\n",
    "# read HTML tables from specific URL with the word \"Country of Birth'\" in them\n",
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/London\", flavor=\"html5lib\", match = 'Country of Birth')\n",
    "print(\"Table count:\", len(tables))\n",
    "print(tables, \"\\n\")\n",
    "\n",
    "# read HTML tables from specific URL with the word \"Division\" in them\n",
    "tables = pd.read_html('https://en.wikipedia.org/wiki/National_Basketball_Association',\n",
    "                    match='Division')\n",
    "\n",
    "# display total number of tables read\n",
    "print(\"Table count:\", len(tables), \"\\n\")\n",
    "\n",
    "# define table\n",
    "df = tables[0]\n",
    "print(tables[0], \"\\n\")\n",
    "\n",
    "# list all column names of table\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://stackabuse.com/reading-and-writing-xml-files-in-python-with-pandas/\n",
    "\n",
    "df = pd.read_xml('sample.xml')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\n",
    "import pandas as pd  \n",
    "  \n",
    "df = pd.read_json(\"Sample.json\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files\n",
    "\n",
    "* pip install beautifulsoup4\n",
    "\n",
    "Also install the requests module (used for making HTTP requests to a specific URL and returns the response)\n",
    "\n",
    "* pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "url = input(\"Enter a website to extract the URL's from: \")\n",
    "\n",
    "# https://www.geeksforgeeks.org/python-programming-language/\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "# print request object\n",
    "print(r.url)\n",
    "   \n",
    "# print status code\n",
    "print(r.status_code)\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "  print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab\n",
    "\n",
    "Go through the following link and run through the steps provided for web scrapping using requests and Beautiful Soup library\n",
    "\n",
    "https://www.geeksforgeeks.org/python-web-scraping-tutorial/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
