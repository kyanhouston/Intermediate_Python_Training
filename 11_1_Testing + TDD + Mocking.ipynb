{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test-Driven Development/Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test-Driven Development\n",
    "* not a library or an API, but rather, TDD is a way of developing software\n",
    "* Python includes awesome support for TDD right out of the box\n",
    "* unit testing has been an integral part of Python since version 2.1 (2001)\n",
    "* numerous improvements since then\n",
    "* no excuse for avoiding testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unit Testing\n",
    "* the smallest testable parts of an application, called units, are individually and independently scrutinized to ensure they work\n",
    "* your functions/methods/procedures should do ONE thing (and do it well)–testing that thing should be relatively easy to explain\n",
    "* exercise the in/out of the unit to be sure it works, especially with corner cases, not just the expected cases\n",
    "* sometimes called \"white box testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Integration Testing\n",
    "* unit testing = testing a single unit of code, isolated from other units\n",
    "* integration testing = exercising 2+ units together, with the goal being to check whether these units have been integrated correctly\n",
    " * if any step fails, the integration test fails, but we must investigate (sometimes deeply) to find out where the failure actually occurred\n",
    " * if unit tests don't pass, there is no point in going further with an integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('TDDflowchart.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TDD is NOT REALLY ABOUT TESTING!\n",
    "* traditionally, unit testing and developer testing are about writing tests to verify the code works…\n",
    "* …whereas main focus of TDD is not about testing\n",
    "* writing a test before the code is implemented changes the way we think when we implement functionality\n",
    " * resulting code is more testable\n",
    " * usually simple, elegant design\n",
    " * easier to read and maintain\n",
    " * why?\n",
    "* so really about writing better code, and we get an automated test suite as a nice side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TDD tests\n",
    "* usually require no setup, vs. traditional unit tests\n",
    "* fast to run, since we run them often during development (sometimes called \"micro tests\")\n",
    "* tests that drive the development forward\n",
    "* not necessarily cover all imaginable scenarios\n",
    " * e.g., file processing function might be tested with a file that exists, a file that's unreadable, a file that doesn't exist, but not necessarily with a 1TB file\n",
    "* \"TDD is about writing better, cleaner, more maintainable code, and only incidentally about testing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TDD Testing Recap\n",
    "* TDD testing general rules\n",
    " * run fast\n",
    " * standalone\n",
    " * independent\n",
    " * run full test suite before/after coding sessions\n",
    " * write a broken unit test when interrupting your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!type addModule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest # Python's unit test module\n",
    "from addModule import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_empty_string():\n",
    "  assert(add('') == 0), \"add with zero error\"\n",
    "  \n",
    "test_add_empty_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_one_number():\n",
    "  assert(add('2') == 2), \"add with one error\"\n",
    "  \n",
    "test_add_one_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_two_numbers_separated_by_comma():\n",
    "  assert(add('3, 4') == 7), \"add with two numbers error\"\n",
    "  \n",
    "test_add_two_numbers_separated_by_comma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_random_numbers_separated_by_comma():  \n",
    "  assert(add('2, 4, 6, 8') == 20), \"add with random numbers error\"\n",
    "\n",
    "test_add_random_numbers_separated_by_comma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_wrong_format_separated_by_comma():  \n",
    "  assert(add('2, \"hello\", 6, 8') == 20), \"add with wrong_formatnumbers error\"\n",
    "\n",
    "test_add_wrong_format_separated_by_comma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test Coverage\n",
    "* before we hand off our code, we want to be sure all tests are passing\n",
    "* Code Coverage = (Number of lines of code executed)/(Total Number of lines of code in a system component) * 100\n",
    "* ...and we have 100% coverage\n",
    "* Coverage.py is a tool for measuring code coverage of Python programs. \n",
    "* pip install coverage\n",
    "\n",
    "https://pypi.org/project/coverage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!type test_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!coverage run -m pytest test_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!coverage report -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a nicer presentation, use coverage html to get annotated HTML listings detailing missed lines:\n",
    "\n",
    "!coverage html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab: Unittest\n",
    "1. Write a method which interacts with a not-yet-implemented library function named prod(), which takes exactly 2 arguments and returns the product of those arguments (they cannot be negative)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
